<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Adult vs Infant classification ML project">
    <meta name="author" content="The Sophia Cofone, Jesse Hautala, Connor Lynch, Zongyu Wu, with thanks to Bootstrap">
    <meta name="generator" content="Hugo 0.88.1">
    <title>DS5110 Faces Project</title>

    <!-- Bootstrap core CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="/docs/5.1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">

    <!-- Favicons -->
    <link rel="apple-touch-icon" href="/docs/5.1/assets/img/favicons/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" href="/docs/5.1/assets/img/favicons/favicon-32x32.png" sizes="32x32" type="image/png">
    <link rel="icon" href="/docs/5.1/assets/img/favicons/favicon-16x16.png" sizes="16x16" type="image/png">
    <link rel="manifest" href="/docs/5.1/assets/img/favicons/manifest.json">
    <link rel="mask-icon" href="/docs/5.1/assets/img/favicons/safari-pinned-tab.svg" color="#7952b3">
    <link rel="icon" href="/docs/5.1/assets/img/favicons/favicon.ico">
    <meta name="theme-color" content="#7952b3">

    <style>
        .bd-placeholder-img {
            font-size: 1.125rem;
            text-anchor: middle;
            -webkit-user-select: none;
            -moz-user-select: none;
            user-select: none;
        }

        @media (min-width: 768px) {
            .bd-placeholder-img-lg {
                font-size: 3.5rem;
            }
        }
    </style>
</head>

<body>
    <nav class="navbar navbar-expand-md navbar-light fixed-top bg-light">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">Classifying Faces: adult vs infant</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
                aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto mb-2 mb-md-0">
                    <li class="nav-item">
                        <a class="nav-link active" aria-current="page" href="#models">Models</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="#analysis">Analysis</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="#acknowledgements">Acknowledgements</a>
                    </li>
                </ul>
            </div>
            <div class="navbar-end-item">
                <a href="https://github.com/ds5110/faces" target="_blank" title="GitHub"
                    class="nav-item align-right mx-2">
                    <img src="github.png" width="32px" />
                </a>
            </div>
        </div>
    </nav>

    <div class="container">
        <header class="d-flex flex-wrap justify-content-center py-3 mb-4 border-bottom">
            <a href="/" class="d-flex align-items-center mb-3 mb-md-0 me-md-auto text-dark text-decoration-none">
                <span class="fs-4">Classifying Faces: adult vs infant</span>
            </a>
            <a href="/" class="nav-item align-right mx-2"></a>
        </header>

        <div class="p-5 mb-4 bg-light rounded-3">
            <div class="container-fluid py-5">
                <h1 class="display-5 fw-bold">Classifying Faces: adult vs infant</h1>
                <p class="col-md-8 fs-4">
                    This project is an exploration into the application of classical machine learning models such as
                    SVC,
                    Logistic Regression, and Naive Bayes to classify adult vs infant faces using facial
                    landmark data. The data used in this analysis is provided by <a
                        href="https://github.com/ostadabbas/Infant-Facial-Landmark-Detection-and-Tracking">InfAnFace:
                        Bridging the Infant--Adult Domain Gap in Facial Landmark Estimation in the Wild</a>.
                </p>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="p-5 mb-4 bg-light rounded-3">
            <div class="container-fluid py-5">
                <h1 class="display-5 fw-bold">Derived Features: Preprocessing and Normalization</h1>
                <p class="md-8 fs-4">
                    In order to reduce the influence of extrinsic factors (e.g. image size, camera angle, and subject pose)
                    we derived additional metadata from the ground truth landmarks for
                    <a href="https://coe.northeastern.edu/Research/AClab/InfAnFace/labels.csv">InfAnFace</a> and
                    <a href="https://github.com/ostadabbas/Infant-Facial-Landmark-Detection-and-Tracking/raw/master/data/300w">300W</a>
                    data. These derived features generally conform much more closely to the type of input statistical
                    learning models are designed to process.
                </p>
                <p class="md-8 fs-4">
                These derived features include:
                </p>
                <ul class="fs-4">
                    <li>Estimated Rotations</li>
                    <li>Box Ratio and Interocular Distance</li>
                    <li>Normalized Landmarks</li>
                    <li>Euclidean Distances</li>
                </ul>
                <p>For the full list of derived features, please see the
                    <a href="https://github.com/ds5110/faces/blob/main/preprocessing.md#derived-features-reference">Derived Features Reference</a>
                    section of the
                    <a href="https://github.com/ds5110/faces/blob/main/preprocessing.md">Preprocessing</a>
                    markdown file.</p>
                <p class="md-8 fs-4">
                Some models struggled to classify images accurately from just raw landmark coordinates,
                    but by using Normalized Landmarks and Estimated Rotations, we were able to
                    reduce the noise in the data (improving model accuracy) and reduce
                    the influence of extrinsic factors, increasing our confidence that the models are making
                    valid distinctions based on meaningful features.
                </p>
                <img class="img-fluid" src="normalized_landmarks.png">
                <p class="md-8 fs-4">
                </p>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="row align-items-md-stretch" id="models">
            <div class="col-md-6">
                <div class="h-100 p-5 bg-light border rounded-3">
                    <h2>PCA > SVC</h2>
                    <h2>High Dimensional</h2>
                    <div class="ratio ratio-4x3">
                        <img src="SVC_landmarks.png" class="img-fluid">
                    </div>
                    <p class="col-md-8 fs-4"></p>
                    <p>Our best "high" dimensional model is a PCA > SVC model with 55 principal components.</p>
                    <p>To generate this result we applied PCA to the normalized landmark coordinates and used SVC for classification. We used cross-validated hyperparameter tuning to find the best number of principal components, SVC regularization and SVC kernel coefficient.
                    </p>
                    <p>For further details on hyperparameter tuning and plotting of the principal components, please
                        visit <a href="https://github.com/ds5110/faces/blob/main/svc.md">svc.md</a>.
                    </p>
                </div>
            </div>
            <div class="col-md-6">
                <div class="h-100 p-5 bg-light border rounded-3">
                    <h2>Logistic Regression</h2>
                    <h2>"Mid" Dimensional</h2>
                    <div class="ratio ratio-4x3">
                        <img src="p3_fd_cmat.png" class="img-fluid">
                    </div>
                    <p class="col-md-8 fs-4"></p>
                    <p>Our best "mid" dimensional model is logistic regression with 7 features.</p>
                    <p>To generate this result, we derived pair-wise euclidian
                        distances from the normalized landmark coordinates.
                        Then, we applied forward feature selection to pick out the 6 most relevant distances.
                        Finally, using recursive feature selection and resampling, we combined the 6 distances with the
                        "box ratio" predictor and applied logistic regression.
                    </p>
                    <p>For further details on hyperparameter tuning and histograms of the distrobutions, please visit <a
                            href="https://github.com/ds5110/faces/blob/main/logreg.md">logreg.md</a>.
                        Please visit <a
                            href="https://github.com/ds5110/faces/blob/main/feature_selection.md">feature_election.md</a>
                        and <a href="https://github.com/ds5110/faces/blob/main/sampling.md">sampling.md</a> for details
                        on those methods.
                    </p>
                </div>
            </div>
            <div class="col-md-6">
                <div class="h-100 p-5 bg-light border rounded-3">
                    <h2>Gaussian Naive Bayes</h2>
                    <h2>Low Dimensional</h2>
                    <div class="ratio ratio-4x3">
                        <img src="bayes_interoc_norm_confusion_matrix.png" class="img-fluid">
                    </div>
                    <p class="col-md-8 fs-4"></p>
                    <p>Some of our best low-dimensional models use just 2 features.</p>
                    <p>This model uses two derived features inspired by the statistics in TABLE II on page 3 of the <a href="https://arxiv.org/pdf/2110.08935.pdf">InfAnFace</a> paper:
                        <ul>
                            <li><b>boxratio</b>: <code>(box_width)/(box_height)</code> (using rotated coordinates)</li>
                            <li><b>interoc</b>: Euclidean distance between landmarks 36, 45 (outer canthi);<br>
                                <i>NOTE:</i> this is similar to <b>interoc_norm</b>, but it is <i>not normalized</i></li>
                        </ul>
                    </p>
                    <p>For further details around this classifier, please visit <a
                            href="https://github.com/ds5110/faces/blob/main/bayes.md">bayes.md</a>.
                    </p>
                </div>
            </div>
            <div class="col-md-6">
                <div class="h-100 p-5 bg-light border rounded-3">
                    <h2>PCA > SVC</h2>
                    <h2>Low Dimensional</h2>
                    <div class="ratio ratio-4x3">
                        <img src="SVC_geometric_value.png" class="img-fluid">
                    </div>
                    <p class="col-md-8 fs-4"></p>
                    <p>Our best "low" dimensional model is SVC with two features inspired by the statistics in TABLE II on page 3 of the <a href="https://arxiv.org/pdf/2110.08935.pdf">InfAnFace</a> paper:
                        <ul>
                            <li><b>boxratio</b>: <code>(box_width)/(box_height)</code> (using rotated coordinates)</li>
                            <li><b>interoc_norm</b>: Euclidean distance between outer canthi (normalized to box extents)</li>
                        </ul>
                    </p>
                    <p>For further details on hyperparameter tuning and plotting of the principal components, please
                        visit <a href="https://github.com/ds5110/faces/blob/main/svc.md">svc.md</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="p-5 mb-4 bg-light rounded-3" id="analysis">
            <div class="container-fluid py-5">
                <h1 class="display-5 fw-bold">Analysis</h1>
                <p class="col-md-8 fs-4">Challenges</p>
                <p>Some of the biggest challenges in this project were around pre-processing and normalizing the data.
                    These landmark data are derived from a 2D projection of a 3D subject and sometimes the subject may
                    be turned or tilted.
                    Sometimes the camera may be closer or further from the subject and sometimes the resolution of the
                    image may be larger or smaller to a significant extent.
                    All of these extrinsic factors can have a large impact on the landmarks' coordinate values.
                    We tried to account for these differences by "normalizing" the landmark points, rotating and scaling
                    the points to the "bounding box" of the face.
                    This allowed us to better compare the faces to one another, and gave us assurance that our
                    classifiers are more likely to distinguish actual features of the faces (rather than those extrinsic
                    features, like scale and rotation).
                    For more information on the specific pre-processing steps done, please refer to <a
                        href="https://github.com/ds5110/faces/blob/main/preprocessing.md">preprocessing.md</a>.
                </p>
                <p class="col-md-8 fs-4">Conclusion</p>
                <p>After testing out a collection of supervised machine learning models with different sets of features,
                    pre-processing steps,
                    resampling, feature selection, and dimensional reduction methods we concluded that
                    classical machine learning is a powerful way to classify facial landmark data.
                    We also found that domain knowledge can help inform feature engineering, which ended up being a
                    critical step in our process.
                    However, we realize that due to the nature of the 2-D data points, there are certain normalizing
                    transformations that we could not do.
                    There also could be other externalities influencing the accuracy scores of our models.
                </p>
                <p class="col-md-8 fs-4">Future work</p>
                <p>We may be able to understand some of our models better by inspecting specific images that are harder
                    to classify correctly.
                    We might find a way to quantify the degree of certainty in our classifiers (e.g. "conformal prediction")
                    and sort misclassified images for review; perhaps a pattern would emerge, indicating a common mode
                    of failure.
                    For example, one of our best predictors across models was <code>boxratio</code>, but we expect this
                    feature to be artificially decreased for extreme angles of rotation about the y-axis; perhaps an
                    infant face in profile (i.e. turned 90 degrees from the camera) would have a ratio indistinct from
                    that of an adult face; perhaps a prioritized list of problematic images would make this evident.
                    We might find ways to improve our API for plotting images, to make this sort of review easier.
                    We would also like to extend our local image caching solution to retrieve <b>300W</b> faces
                    as easily as the <b>InfAnFace</b> images.
                </p>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="p-10 mb-6 bg-light rounded-3" id="acknowledgements">
            <div class="container-fluid py-5">
                <p class="col-md-8 fw-bold fs-4">Acknowledgements</p>
                <p>For more information about the contributors of this project, please visit the <a
                        href="https://github.com/ds5110/faces">project repository</a>.</p>
                <p>The data used in this analysis comes from <a
                        href="https://github.com/ostadabbas/Infant-Facial-Landmark-Detection-and-Tracking">InfAnFace:
                        Bridging the Infant--Adult Domain Gap in Facial Landmark Estimation in the Wild</a>.
                </p>
                <p>Wan, M., Zhu, S., Luan, L., Prateek, G., Huang, X., Schwartz-Mette, R., Hayes, M., Zimmerman, E.,
                    & Ostadabbas, S. "InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation
                    in the wild." 26th International Conference on Pattern Recognition (ICPR 2022).</p>
                <p>Additional thanks to Dr. Michael Wan for his feedback and guidance on our project.</p>
            </div>
        </div>
    </div>

    <footer class="pt-3 mt-4 text-muted border-top">
        &copy; 2022 DS5110
    </footer>

</body>

</html>